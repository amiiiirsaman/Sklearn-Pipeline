{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import expectexception  #  Used to specify if an exception *should* occur (for notebook testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "<!-- requirement: images/PML_feature_union.svg -->\n",
    "<!-- requirement: small_data/DC_properties.csv -->\n",
    "\n",
    "# Scikit-learn Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "While scikit-learn provides a number of powerful, built-in transformers and predictors, we sometimes need custom functionality, especially for preprocessing and data wrangling. The data I deal with in the real world does not often come to us in a clean ready-to-use format. \n",
    "\n",
    "In this notebook I will demonstrate how to process data that is in a format which is unsuitable for scikit-learn and transform it into a 2D matrix that scikit-learn predictors expect. I'll be working a data set that consists of house prices in the Washington, D.C. area, together with various features about those houses. This data is stored as a CSV file.  \n",
    "\n",
    "> **Note:** I obtained this data from [this website](https://www.kaggle.com/christophercorrea/dc-residential-properties) under a [Creative Commons CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/), and have modified the data somewhat ourselves before making it available here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The next block of code reads the CSV file and loads it into a pandas DataFrame object. Note that I know that one of the columns contains a date/time string, so I have told pandas to treat that accordingly when we load the CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('small_data/DC_properties.csv', parse_dates=['SALEDATE'], low_memory=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Examining the first element in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "      <th>ROOMS</th>\n",
       "      <th>BATHRM</th>\n",
       "      <th>HF_BATHRM</th>\n",
       "      <th>BEDRM</th>\n",
       "      <th>KITCHENS</th>\n",
       "      <th>FIREPLACES</th>\n",
       "      <th>LANDAREA</th>\n",
       "      <th>EYB</th>\n",
       "      <th>SALEDATE</th>\n",
       "      <th>SALE_NUM</th>\n",
       "      <th>HEAT</th>\n",
       "      <th>AC</th>\n",
       "      <th>QUALIFIED</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ASSESSMENT_NBHD</th>\n",
       "      <th>QUADRANT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1095000.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1680</td>\n",
       "      <td>1972</td>\n",
       "      <td>2003-11-25</td>\n",
       "      <td>1</td>\n",
       "      <td>Warm Cool</td>\n",
       "      <td>Y</td>\n",
       "      <td>Q</td>\n",
       "      <td>Residential</td>\n",
       "      <td>20009</td>\n",
       "      <td>38.91468</td>\n",
       "      <td>-77.040832</td>\n",
       "      <td>Old City 2</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRICE  ROOMS  BATHRM  HF_BATHRM  BEDRM  KITCHENS  FIREPLACES  LANDAREA  \\\n",
       "0  1095000.0      8       4          0      4       2.0           5      1680   \n",
       "\n",
       "    EYB   SALEDATE  SALE_NUM       HEAT AC QUALIFIED       SOURCE  ZIPCODE  \\\n",
       "0  1972 2003-11-25         1  Warm Cool  Y         Q  Residential    20009   \n",
       "\n",
       "   LATITUDE  LONGITUDE ASSESSMENT_NBHD QUADRANT  \n",
       "0  38.91468 -77.040832      Old City 2       NW  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Notice that this data set contains information about the houses and for every observation in the data set I\n",
    "have several features ranging from categorical features of the houses, as well as numeric features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "y = data['PRICE'].values\n",
    "X = data.loc[:, 'ROOMS':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROOMS</th>\n",
       "      <th>BATHRM</th>\n",
       "      <th>HF_BATHRM</th>\n",
       "      <th>BEDRM</th>\n",
       "      <th>KITCHENS</th>\n",
       "      <th>FIREPLACES</th>\n",
       "      <th>LANDAREA</th>\n",
       "      <th>EYB</th>\n",
       "      <th>SALEDATE</th>\n",
       "      <th>SALE_NUM</th>\n",
       "      <th>HEAT</th>\n",
       "      <th>AC</th>\n",
       "      <th>QUALIFIED</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ASSESSMENT_NBHD</th>\n",
       "      <th>QUADRANT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1680</td>\n",
       "      <td>1972</td>\n",
       "      <td>2003-11-25</td>\n",
       "      <td>1</td>\n",
       "      <td>Warm Cool</td>\n",
       "      <td>Y</td>\n",
       "      <td>Q</td>\n",
       "      <td>Residential</td>\n",
       "      <td>20009</td>\n",
       "      <td>38.914680</td>\n",
       "      <td>-77.040832</td>\n",
       "      <td>Old City 2</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1680</td>\n",
       "      <td>1984</td>\n",
       "      <td>2016-06-21</td>\n",
       "      <td>3</td>\n",
       "      <td>Hot Water Rad</td>\n",
       "      <td>Y</td>\n",
       "      <td>Q</td>\n",
       "      <td>Residential</td>\n",
       "      <td>20009</td>\n",
       "      <td>38.914684</td>\n",
       "      <td>-77.040678</td>\n",
       "      <td>Old City 2</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1680</td>\n",
       "      <td>1984</td>\n",
       "      <td>2006-07-12</td>\n",
       "      <td>1</td>\n",
       "      <td>Hot Water Rad</td>\n",
       "      <td>Y</td>\n",
       "      <td>Q</td>\n",
       "      <td>Residential</td>\n",
       "      <td>20009</td>\n",
       "      <td>38.914683</td>\n",
       "      <td>-77.040629</td>\n",
       "      <td>Old City 2</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROOMS  BATHRM  HF_BATHRM  BEDRM  KITCHENS  FIREPLACES  LANDAREA   EYB  \\\n",
       "0      8       4          0      4       2.0           5      1680  1972   \n",
       "1      9       3          1      5       2.0           4      1680  1984   \n",
       "2      8       3          1      5       2.0           3      1680  1984   \n",
       "\n",
       "    SALEDATE  SALE_NUM           HEAT AC QUALIFIED       SOURCE  ZIPCODE  \\\n",
       "0 2003-11-25         1      Warm Cool  Y         Q  Residential    20009   \n",
       "1 2016-06-21         3  Hot Water Rad  Y         Q  Residential    20009   \n",
       "2 2006-07-12         1  Hot Water Rad  Y         Q  Residential    20009   \n",
       "\n",
       "    LATITUDE  LONGITUDE ASSESSMENT_NBHD QUADRANT  \n",
       "0  38.914680 -77.040832      Old City 2       NW  \n",
       "1  38.914684 -77.040678      Old City 2       NW  \n",
       "2  38.914683 -77.040629      Old City 2       NW  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98216 entries, 0 to 98215\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   ROOMS            98216 non-null  int64         \n",
      " 1   BATHRM           98216 non-null  int64         \n",
      " 2   HF_BATHRM        98216 non-null  int64         \n",
      " 3   BEDRM            98216 non-null  int64         \n",
      " 4   KITCHENS         97979 non-null  float64       \n",
      " 5   FIREPLACES       98216 non-null  int64         \n",
      " 6   LANDAREA         98216 non-null  int64         \n",
      " 7   EYB              98216 non-null  int64         \n",
      " 8   SALEDATE         98216 non-null  datetime64[ns]\n",
      " 9   SALE_NUM         98216 non-null  int64         \n",
      " 10  HEAT             98216 non-null  object        \n",
      " 11  AC               98216 non-null  object        \n",
      " 12  QUALIFIED        98216 non-null  object        \n",
      " 13  SOURCE           98216 non-null  object        \n",
      " 14  ZIPCODE          98216 non-null  int64         \n",
      " 15  LATITUDE         98216 non-null  float64       \n",
      " 16  LONGITUDE        98216 non-null  float64       \n",
      " 17  ASSESSMENT_NBHD  98216 non-null  object        \n",
      " 18  QUADRANT         98115 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(3), int64(9), object(6)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Remark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "I am going to be discussing various scikit-learn classes in this notebook. It is highly recommended to spend some time reading the [documentation](https://scikit-learn.org/stable/index.html) on new classes as you encounter them for the first time.  The scikit-learn documentation is very thorough, informative, and has good examples included in it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let us focus first on some of the categorical feature from the data set. These categorical features include columns such as `HEAT`, `AC` and `SOURCE`.  `HEAT` specifies a particular type of heating equipment that is installed in the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Air Exchng',\n",
       " 'Air-Oil',\n",
       " 'Elec Base Brd',\n",
       " 'Electric Rad',\n",
       " 'Evp Cool',\n",
       " 'Forced Air',\n",
       " 'Gravity Furnac',\n",
       " 'Hot Water Rad',\n",
       " 'Ht Pump',\n",
       " 'Ind Unit',\n",
       " 'No Data',\n",
       " 'Wall Furnace',\n",
       " 'Warm Cool',\n",
       " 'Water Base Brd']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(X['HEAT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.,  2.],\n",
       "       [ 7.,  2.],\n",
       "       [ 7.,  2.],\n",
       "       [ 7.,  2.],\n",
       "       [ 7.,  2.],\n",
       "       [12.,  2.],\n",
       "       [12.,  2.],\n",
       "       [12.,  2.],\n",
       "       [ 7.,  2.],\n",
       "       [ 7.,  2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "le = OrdinalEncoder()\n",
    "le.fit_transform(X[['HEAT', 'AC']])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<98216x19 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 294648 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform(X[['HEAT', 'AC', 'SOURCE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Do not call .toarray() on the full size sparse matrix!!  It will crash your kernel!\n",
    "\n",
    "ohe.fit_transform(X[['HEAT', 'AC', 'SOURCE']])[0:3, :].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "I shall very soon how we can use the `ColumnTransformer` to utilize several transformers on columns and put the results together into a feature matrix. Let's use [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).  I have added additional categorical features below to the ones we have mentioned above.  \n",
    "\n",
    "Note that even though the `ZIPCODE` is stored as a numeric value, we are encoding it as a categorical feature for similar reasons described above, i.e. there is no reason that an increase in the value of `ZIPCODE` should necessarily correspond to an increase/decrease in the house price.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('categorical', OneHotEncoder()), ('regressor', Ridge())])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "categorical_columns = ['HEAT', 'AC', 'SOURCE', 'QUALIFIED', 'ZIPCODE', 'ASSESSMENT_NBHD']\n",
    "\n",
    "est = Pipeline([\n",
    "    ('categorical', OneHotEncoder()),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "est.fit(X[categorical_columns], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "I can get predictions in the usual fashion by feeding in appropriate data. Since I trained on only the categorical columns, I only feed in the categorical columns when we make predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -4496.41871641,  -29127.27267785,  -29127.27267785,\n",
       "        -29127.27267785,  -29127.27267785,   -4496.41871641,\n",
       "         -4496.41871641,   -4496.41871641,  -29127.27267785,\n",
       "       1796722.49619058])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.predict(X[categorical_columns])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "I see that our model isn't very good, giving us negative values for the predictions of some house prices. There can be many reasons for this, but it can be that we are only using these categorical features of the data, and I am trying to fit a linear model when the categorical features have inherently non-linear relationships to the prices.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score using selected columns and transformers: 0.15178909180404088\n"
     ]
    }
   ],
   "source": [
    "print(f'R^2 score using selected columns and transformers: {est.score(X[categorical_columns], y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "I have some \"signal\" from the categorical variables alone.  Adding numeric features should give us a better score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There are many other encoding methods, but one-hot encoding is the most common and is usually a fine choice. If you're interested in alternatives, you may want to look at the [category encoder package](http://contrib.scikit-learn.org/category_encoders/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Combining features using `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "I have categorical features above, and I know how to one-hot encode them. How can we combine them together with numeric features such as `LANDAREA`, `ROOMS`, and `BEDRM`?  \n",
    "\n",
    "The [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) lets us combine together outputs from transformers on different columns into a single feature matrix.  `ColumnTransformer` works similarly to `Pipeline`, but we give tuples of three elements, the first being a \"name\" for the step, the second a transformer, and the third part of the tuple is a list of columns to which the transformer should apply.  If the input matrix is a DataFrame, we can use the names of the columns.  If the input is a numpy array (or also a DataFrame), we can specify columns by numeric indices.  If we want to just \"pass through\" the values of the columns, i.e. get the values without transforming them, we can use `passthrough` in place of a transformer.  \n",
    "\n",
    "Here's how we can use `ColumnTransformer` to combine the one-hot encoded categorical features above with the (unaltered) values of several numeric features we have selected.  Note that the output is always a numpy array or sparse array regardless of the format of the input feature matrix.  Also, the order of the columns in the output is governed by the order of the transformers and specified columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score using selected columns and transformers: 0.14272254717554334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_columns = ['ROOMS', 'BATHRM', 'HF_BATHRM', 'BEDRM', 'EYB', 'FIREPLACES', \n",
    "                   'LANDAREA', 'SALE_NUM', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "features = ColumnTransformer([\n",
    "    ('categorical', OneHotEncoder(), categorical_columns),\n",
    "    ('numeric', 'passthrough', numeric_columns)\n",
    "])\n",
    "\n",
    "est = Pipeline([\n",
    "    ('features', features),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "est.fit(X, y)\n",
    "\n",
    "print(f'R^2 score using selected columns and transformers: {est.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Remark:**  Even though I have only used a subset of the columns in building the new feature matrix, because of the way the `ColumnTransformer` is implemented, I will receive an error if I feed in a feature matrix that has fewer columns than the training data, even if all the columns used in all transformers are present.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HEAT', 'AC', 'SOURCE', 'QUALIFIED', 'ZIPCODE', 'ASSESSMENT_NBHD', 'ROOMS', 'BATHRM', 'HF_BATHRM', 'BEDRM', 'EYB', 'FIREPLACES', 'LANDAREA', 'SALE_NUM', 'LATITUDE', 'LONGITUDE']\n"
     ]
    }
   ],
   "source": [
    "used_columns = categorical_columns + numeric_columns\n",
    "print(used_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-17-c368f5d05b46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[1;33m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mused_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    406\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n",
      "\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 581\u001b[1;33m             raise ValueError('Number of features of the input must be equal '\n",
      "\u001b[0m\u001b[0;32m    582\u001b[0m                              \u001b[1;34m'to or greater than that of the fitted '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    583\u001b[0m                              \u001b[1;34m'transformer. Transformer n_features is {0} '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the input must be equal to or greater than that of the fitted transformer. Transformer n_features is 19 and input n_features is 16.\n"
     ]
    }
   ],
   "source": [
    "%%expect_exception ValueError\n",
    "\n",
    "est.predict(X[used_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Building custom scikit-learn classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "I can build custom classes as extensions of existing scikit-learn classes. As I have already learned, there are two main types of classes in scikit-learn: predictors and transformers. \n",
    "\n",
    "Predictors, such as `LinearRegression` or `RandomForestClassifier`, tend to represent the final step in a machine learning model, such as performing regression or classification.\n",
    "\n",
    "Transformers, such as `StandardScaler`, are steps for preprocessing and transforming data, and in a pipeline they precede predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "All transformers in scikit-learn support `fit` and `transform` methods and implement the following interface:\n",
    "``` python\n",
    "class Transformer(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, ...):\n",
    "    # initialization code\n",
    "    \n",
    "  def fit(self, X, y=None):\n",
    "    # fit the transformer\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X):\n",
    "    # apply the transformation\n",
    "    return ...\n",
    "```\n",
    "\n",
    "Note that for transformers, `fit` is often empty and only `transform` actually does something. In general, the `fit` method contains the code for the transformer to learn parameters from the training data that can then be used during the data transformation process. Note, all transformers need to return `self` in the `fit` method to be compatible with the scikit-learn infrastructure.\n",
    "\n",
    "Conforming to the convention outline here has the benefit that many tools (e.g. pipelines, cross-validation, grid search) rely on this interface so you can use your new transformers with the existing scikit-learn infrastructure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "All predictors (e.g. `LinearRegression`, `DecisionTreeRegressor`, etc ...) support `fit` and `predict` methods.  You can build your own predictor, for example, for regression by using the following template. \n",
    "``` python                                                                                                                                        \n",
    "class Predictor(BaseEstimator, RegressorMixin):\n",
    "  def __init__(self, ...):\n",
    "    # initialization code\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    # fit the model ...\n",
    "    return self\n",
    "    \n",
    "  def predict(self, X):\n",
    "    # make predictions \n",
    "    return ...\n",
    "```\n",
    "``` python    \n",
    "  def score(self, X, y):\n",
    "    # custom score implementation\n",
    "    # this is optional, if not defined default is R^2\n",
    "    return ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Building a custom transformer using LATITUDE and LONGITUDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We included `LATITUDE` and `LONGITUDE` in our numeric features above.  While they are certainly continuous features, so worthy to include there, we can also explore other uses of these two values.  For example, is the distance to the White House relevant to the sales price of a house?  Does it affect the price of the house if there is a nearby airport?  Georgetown is a rather affluent part of Washington, D.C., is home to Georgetown University, and the port of Georgetown predated the establishment of Washington, D.C. by 40 years or so.  Columbia Heights is known for its diversity, major retailers, and is home to Howard University.  And so forth.  \n",
    "\n",
    "Let's build a custom transformer that will take in some latitude/longitude pairs and compute distances (\"as the crow flies\") between the `LATITUDE` and `LONGITUDE` data and the given pair(s), allowing us to engineer distances from any point that we want to do so.  \n",
    "\n",
    "To define a custom transformer, we follow the outline above by defining the `__init__`, `fit` and `transform` methods.  \n",
    "The `__init__` method just needs to store the data supplied in the constructor, while the `fit` method doesn't need to \"learn\" anything and so only needs to `return self`.  \n",
    "\n",
    "The `transform` method will do the actual job of computing the distances.  We assume that the input to the constructor is a list of lists/tuples, and the feature matrix the transformer receives is in the form of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Will create distances from a feature matrix.  \n",
    "        locations is a list of tuples/lists of latitude/longitude pairs\n",
    "    Assumes the feature matrix X is a DataFrame with columns 'LATITUDE and 'LONGITUDE'\n",
    "    Returns a numpy array of distances between each location and the lat/long pairs in X\n",
    "    \"\"\"\n",
    "    def __init__(self, locations):\n",
    "        self.locations = locations\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        lat_long = X[['LATITUDE', 'LONGITUDE']].values\n",
    "        result = []\n",
    "        for place in self.locations:\n",
    "            result.append(np.sqrt(((lat_long-place)**2).sum(axis=1)).reshape(-1,1))\n",
    "            \n",
    "        return np.hstack(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's pick a few places to use in our `DistanceTransformer` and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00977192, 0.03533652, 0.41606692, 0.01726025],\n",
       "       [0.00985719, 0.03549053, 0.41622045, 0.01722594],\n",
       "       [0.00988622, 0.03553923, 0.41626928, 0.01721338],\n",
       "       [0.01071615, 0.03642903, 0.41720663, 0.01667518],\n",
       "       [0.00962061, 0.03609586, 0.4167171 , 0.01781187]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMBIA_HEIGHTS = (38.922741, -77.046356)\n",
    "GEORGETOWN_UNIVERSITY = (38.912180, -77.076080)\n",
    "DULLES_AIRPORT = (38.944444, -77.455833)\n",
    "WHITE_HOUSE = (38.897957, -77.036560)\n",
    "\n",
    "locations = [COLUMBIA_HEIGHTS, GEORGETOWN_UNIVERSITY, DULLES_AIRPORT, WHITE_HOUSE]\n",
    "\n",
    "dist = DistanceTransformer(locations)\n",
    "\n",
    "dist.fit_transform(X[['LATITUDE','LONGITUDE']][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And then we can add it to our feature matrix and try our regression one more time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score using selected columns and transformers: 0.14577596541515392\n"
     ]
    }
   ],
   "source": [
    "features = ColumnTransformer([\n",
    "    ('categorical', OneHotEncoder(), categorical_columns),\n",
    "    ('numeric', 'passthrough', numeric_columns),\n",
    "    ('distances', DistanceTransformer(locations), ['LATITUDE', 'LONGITUDE'])\n",
    "])\n",
    "\n",
    "est = Pipeline([\n",
    "    ('features', features),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "est.fit(X,y)\n",
    "\n",
    "print(f'R^2 score using selected columns and transformers: {est.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The extra features are only giving us a slightly better performance in this case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Using `FunctionTransformer` to build stateless transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "If we want to use the `SALEDATE` in a linear model, we will also need to translate this into a numeric value as this field is a date/time.  To do this transformation, we can make use of scikit-learn's [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html).  As the documentation states, `FunctionTransformer`  is designed to \"construct a transformer from an arbitrary callable\".  This is useful for a \"stateless\" transformation such as taking the logarithm of a column, performing a custom scaling method, etc.  \n",
    "\n",
    "Each element in the `SALEDATE` column is a pandas [`Timestamp`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html#pandas.Timestamp) object, which is the pandas equivalent of Python's [`datetime`](https://docs.python.org/3/library/datetime.html) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2003-11-25 00:00:00')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[0, 'SALEDATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We will want to convert these timestamps to the so-called \"Unix epoch time\", or the number of seconds since midnight Jan 1, 1970.  We could use any \"zero reference\" point in time, but this will serve our purposes for use in a linear regressor. There are several approaches to convert a pandas `Timestamp` to Unix epoch time. We'll use the method detailed in the official pandas [documenation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#from-timestamps-to-epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1069718400\n",
       "1        1466467200\n",
       "2        1152662400\n",
       "3        1267142400\n",
       "4        1317254400\n",
       "            ...    \n",
       "98211    1257984000\n",
       "98212    1428019200\n",
       "98213    1380844800\n",
       "98214    1222732800\n",
       "98215    1428969600\n",
       "Name: SALEDATE, Length: 98216, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X['SALEDATE'] - pd.Timestamp('1970-01-01')) // pd.Timedelta('1s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "To use the `FunctionTransformer`, we need to define a function that will take a collection of `datetime` objects and returns an \"array-like\" structure of corresponding epoch times.  We will take advantage of the fact that we know the input will be a pandas Series of `datetime` objects, as we will use the result inside of a `ColumnTransformer` to add this to our feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def to_epoch(series_of_times):\n",
    "    \"\"\"\n",
    "    Assumes the input is a pandas Series of datetime objects.\n",
    "    Returns a numpy array of Unix epoch times, measured as seconds since midnight Jan 1, 1970.\n",
    "    \"\"\"\n",
    "    return ((series_of_times - pd.Timestamp('1970-01-01')) // pd.Timedelta('1s')).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "A quick check that it works the way we need it to work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1069718400],\n",
       "       [1466467200],\n",
       "       [1152662400],\n",
       "       [1267142400],\n",
       "       [1317254400]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_epoch(X['SALEDATE'][0:5])  # seconds since midnight Jan 1, 1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And making it into a transformer we can use in a pipeline, with a small test first.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1069718400],\n",
       "       [1466467200],\n",
       "       [1152662400],\n",
       "       [1267142400],\n",
       "       [1317254400]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "date_transformer = FunctionTransformer(to_epoch)\n",
    "date_transformer.transform(X['SALEDATE'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Then we can add this newly transformed column to our feature matrix and re-fit the regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score using selected columns and transformers: 8.626013346180184e-05\n"
     ]
    }
   ],
   "source": [
    "features = ColumnTransformer([\n",
    "    ('categorical', OneHotEncoder(), categorical_columns),\n",
    "    ('numeric', 'passthrough', numeric_columns),\n",
    "    ('distances', DistanceTransformer(locations), ['LATITUDE', 'LONGITUDE']),\n",
    "    ('dates', date_transformer, 'SALEDATE')\n",
    "])\n",
    "\n",
    "est = Pipeline([\n",
    "    ('features', features),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "est.fit(X,y)\n",
    "\n",
    "print(f'R^2 score using selected columns and transformers: {est.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Our score actually decreased here, a lot!  Why?  \n",
    "\n",
    "The transformed date/time values are on a large scale, and the distances computed by the `DistanceTransformer` are on a much smaller scale.  All of the one-hot encoded values are either 0 or 1 (by definition).  So the features in the new feature matrix live on very different scales to one another which leads to some numerical instabilities in the analytic solution to the linear regression.  \n",
    "\n",
    "So let's try scaling the values in our feature matrix.  As mentioned in the last notebook, we would often use `StandardScaler` here, but this won't work (with the default parameters) in this case as our transformed feature matrix is a sparse array.  This is because subtracting the mean of each column (a usual step in this transformation) will turn a sparse matrix into a dense matrix, usually giving a very large feature matrix that won't fit into the memory of our computer.  \n",
    "\n",
    "Let's use the [`MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) as a final transformer on all the data in the final feature matrix, right before the regressor.  This transformer will transform the data such that the maximal (absolute) value of each feature is 1 and *can* be applied to sparse matrices.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score using selected columns and transformers: 0.15662689565093602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "est = Pipeline([\n",
    "    ('features', features),\n",
    "    ('scaling', MaxAbsScaler()),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "est.fit(X,y)\n",
    "\n",
    "print(f'R^2 score using selected columns and transformers: {est.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Feature unions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "As we have seen, we often need to preprocess different features with different transformers. We saw how the `ColumnTransformer` gives us one way to combined different transformers applied to different columns of an input feature matrix.  Sometimes it is also useful to have another way of combining feature matrices together into a single matrix, and this is what the [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) class helps us to accomplish. \n",
    "\n",
    "![feature union](images/PML_feature_union.svg)\n",
    "\n",
    "We could have built a number of independent transformer pipelines and pasted together the results using `FeatureUnion`.  That wasn't necessary in this case, but can sometimes be useful.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Imputation\n",
    "\n",
    "When data is missing, it's often preferable to impute or artificially assign values to empty fields rather than disregarding incomplete observations entirely. This is especially important when we expect the model we are training to be applied in situations with incomplete information.\n",
    "\n",
    "Scikit-learn offers the [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) transformer, which replaces missing values (instances of `np.nan`) with the average value of the appropriate feature (choose your preferred definition of 'average' using the `strategy` argument). More sophisticated imputation is better preformed in NumPy or Pandas (or writing your own custom transformer to do the job), but `SimpleImputer` has the advantage of convenience. Being a transformer means that it is easy to reuse, behaves consistently, and can be incorporated into pipelines.\n",
    "\n",
    "For the D.C. property data, we have used all of the columns except for `KITCHENS` and `QUADRANT`.  Both of these columns have missing values, so we can't use them directly in a predictor since the scikit-learn predictors can't use missing values.  We can demonstrate how to use the `SimpleImputer` to fill in missing values for `KITCHENS`.  \n",
    "\n",
    "First, let's look at the values that are present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     87784\n",
       "2.0      7656\n",
       "4.0      1859\n",
       "3.0       627\n",
       "NaN       237\n",
       "0.0        43\n",
       "5.0         5\n",
       "6.0         4\n",
       "44.0        1\n",
       "Name: KITCHENS, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['KITCHENS'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We will add to the `ColumnTransformer` that is generating the features, by including an instance of `SimpleImputer` to fill in missing data for `KITCHENS`.  \n",
    "\n",
    "While not 100% accurate, it is reasonable to assume that each property has at least one kitchen in it, as only a tiny fraction (43/98216, less than 0.05%) listed above do not have one.  `SimpleImputer` lets us specify a \"strategy\" to use for imputation.  In this case we will use the `most_frequent` strategy, meaning that missing values will be filled in with 1 (the most frequent occurence) in this case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score using selected columns and transformers: 0.15696113614325713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "features = ColumnTransformer([\n",
    "    ('categorical', OneHotEncoder(), categorical_columns),\n",
    "    ('numeric', 'passthrough', numeric_columns),\n",
    "    ('distances', DistanceTransformer(locations), ['LATITUDE', 'LONGITUDE']),\n",
    "    ('dates', date_transformer, 'SALEDATE'),\n",
    "    ('fill_kitchens', SimpleImputer(strategy='most_frequent'), ['KITCHENS'])\n",
    "])\n",
    "\n",
    "est = Pipeline([\n",
    "    ('features', features),\n",
    "    ('scaling', MaxAbsScaler()),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "est.fit(X,y)\n",
    "\n",
    "print(f'R^2 score using selected columns and transformers: {est.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Note:**  Astute observers might notice a seeming discrepancy in the `features` transformer above.  For the `date_transformer` we specified a scaler value for the column (`SALEDATE`), whereas for the `SimpleImputer` we gave a list with one value in it, namely `['KITCHENS']`.  This is because *we wrote* the `to_epoch` (and hence the `date_transformer`) and did it in such a fashion that the input is expected as a 1-dimensional array (a pandas Series).  \n",
    "\n",
    "On the other hand, `SimpleImputer` is expecting 2-dimensional input.  Specifying the input as a Python list of one item is analogous to passing the (2-dimensional) DataFrame `X[['KITCHENS']]` (consisting of only a single column) to the imputer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## A fancy display...\n",
    "\n",
    "Scikit-learn has a useful way of displaying a `Pipeline` (or, say, a `ColumnTransformer`).  To see this display in a Jupyter notebook, we can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2185b796-746f-4d9e-8bd3-609358eb7212\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2185b796-746f-4d9e-8bd3-609358eb7212\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('features',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  OneHotEncoder(),\n",
       "                                                  ['HEAT', 'AC', 'SOURCE',\n",
       "                                                   'QUALIFIED', 'ZIPCODE',\n",
       "                                                   'ASSESSMENT_NBHD']),\n",
       "                                                 ('numeric', 'passthrough',\n",
       "                                                  ['ROOMS', 'BATHRM',\n",
       "                                                   'HF_BATHRM', 'BEDRM', 'EYB',\n",
       "                                                   'FIREPLACES', 'LANDAREA',\n",
       "                                                   'SALE_NUM', 'LATITUDE',\n",
       "                                                   'LONGITUDE']),\n",
       "                                                 ('distances',\n",
       "                                                  DistanceTransformer(locations=[(38.922741,\n",
       "                                                                                  -77.046356),\n",
       "                                                                                 (38.91218,\n",
       "                                                                                  -77.07608),\n",
       "                                                                                 (38.944444,\n",
       "                                                                                  -77.455833),\n",
       "                                                                                 (38.897957,\n",
       "                                                                                  -77.03656)]),\n",
       "                                                  ['LATITUDE', 'LONGITUDE']),\n",
       "                                                 ('dates',\n",
       "                                                  FunctionTransformer(func=<function to_epoch at 0x7f037740da60>),\n",
       "                                                  'SALEDATE'),\n",
       "                                                 ('fill_kitchens',\n",
       "                                                  SimpleImputer(strategy='most_frequent'),\n",
       "                                                  ['KITCHENS'])])),\n",
       "                ('scaling', MaxAbsScaler()), ('regressor', Ridge())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f625780a-217d-42e9-ae24-eda02af0b00c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f625780a-217d-42e9-ae24-eda02af0b00c\">features: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('categorical', OneHotEncoder(),\n",
       "                                 ['HEAT', 'AC', 'SOURCE', 'QUALIFIED',\n",
       "                                  'ZIPCODE', 'ASSESSMENT_NBHD']),\n",
       "                                ('numeric', 'passthrough',\n",
       "                                 ['ROOMS', 'BATHRM', 'HF_BATHRM', 'BEDRM',\n",
       "                                  'EYB', 'FIREPLACES', 'LANDAREA', 'SALE_NUM',\n",
       "                                  'LATITUDE', 'LONGITUDE']),\n",
       "                                ('distances',\n",
       "                                 DistanceTransformer(locations=[(38.922741,\n",
       "                                                                 -77.046356),\n",
       "                                                                (38.91218,\n",
       "                                                                 -77.07608),\n",
       "                                                                (38.944444,\n",
       "                                                                 -77.455833),\n",
       "                                                                (38.897957,\n",
       "                                                                 -77.03656)]),\n",
       "                                 ['LATITUDE', 'LONGITUDE']),\n",
       "                                ('dates',\n",
       "                                 FunctionTransformer(func=<function to_epoch at 0x7f037740da60>),\n",
       "                                 'SALEDATE'),\n",
       "                                ('fill_kitchens',\n",
       "                                 SimpleImputer(strategy='most_frequent'),\n",
       "                                 ['KITCHENS'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"04c50eab-306f-447d-9102-c7fa389e2d2c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"04c50eab-306f-447d-9102-c7fa389e2d2c\">categorical</label><div class=\"sk-toggleable__content\"><pre>['HEAT', 'AC', 'SOURCE', 'QUALIFIED', 'ZIPCODE', 'ASSESSMENT_NBHD']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"01c72dc2-dba7-45f0-8ea4-cf1fb0bbb261\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"01c72dc2-dba7-45f0-8ea4-cf1fb0bbb261\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"480bd9a0-1c0c-4522-9ff2-c1e1354959de\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"480bd9a0-1c0c-4522-9ff2-c1e1354959de\">numeric</label><div class=\"sk-toggleable__content\"><pre>['ROOMS', 'BATHRM', 'HF_BATHRM', 'BEDRM', 'EYB', 'FIREPLACES', 'LANDAREA', 'SALE_NUM', 'LATITUDE', 'LONGITUDE']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"68ee221e-cdce-45b8-b475-0b7146218488\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"68ee221e-cdce-45b8-b475-0b7146218488\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f7f150b7-d065-4130-8ee4-4f7886088a57\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f7f150b7-d065-4130-8ee4-4f7886088a57\">distances</label><div class=\"sk-toggleable__content\"><pre>['LATITUDE', 'LONGITUDE']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"37952126-2ba2-4dfb-ac39-f7d45eeae0ee\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"37952126-2ba2-4dfb-ac39-f7d45eeae0ee\">DistanceTransformer</label><div class=\"sk-toggleable__content\"><pre>DistanceTransformer(locations=[(38.922741, -77.046356), (38.91218, -77.07608),\n",
       "                               (38.944444, -77.455833),\n",
       "                               (38.897957, -77.03656)])</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b511eef6-41f8-4aaf-b514-8242db7fe527\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b511eef6-41f8-4aaf-b514-8242db7fe527\">dates</label><div class=\"sk-toggleable__content\"><pre>SALEDATE</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5a2804ee-1b65-4330-8a4f-15fca439a1f1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5a2804ee-1b65-4330-8a4f-15fca439a1f1\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=<function to_epoch at 0x7f037740da60>)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"99cf05bd-c6d4-4b51-b326-e2541ce6526a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"99cf05bd-c6d4-4b51-b326-e2541ce6526a\">fill_kitchens</label><div class=\"sk-toggleable__content\"><pre>['KITCHENS']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c089a840-a802-4cfe-90ff-8904ac616756\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c089a840-a802-4cfe-90ff-8904ac616756\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy='most_frequent')</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"656cceba-9fcd-4361-96c4-ca662e66ac93\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"656cceba-9fcd-4361-96c4-ca662e66ac93\">MaxAbsScaler</label><div class=\"sk-toggleable__content\"><pre>MaxAbsScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"54b9d6b3-b585-4819-bf04-f3bc4638eeff\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"54b9d6b3-b585-4819-bf04-f3bc4638eeff\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  OneHotEncoder(),\n",
       "                                                  ['HEAT', 'AC', 'SOURCE',\n",
       "                                                   'QUALIFIED', 'ZIPCODE',\n",
       "                                                   'ASSESSMENT_NBHD']),\n",
       "                                                 ('numeric', 'passthrough',\n",
       "                                                  ['ROOMS', 'BATHRM',\n",
       "                                                   'HF_BATHRM', 'BEDRM', 'EYB',\n",
       "                                                   'FIREPLACES', 'LANDAREA',\n",
       "                                                   'SALE_NUM', 'LATITUDE',\n",
       "                                                   'LONGITUDE']),\n",
       "                                                 ('distances',\n",
       "                                                  DistanceTransformer(locations=[(38.922741,\n",
       "                                                                                  -77.046356),\n",
       "                                                                                 (38.91218,\n",
       "                                                                                  -77.07608),\n",
       "                                                                                 (38.944444,\n",
       "                                                                                  -77.455833),\n",
       "                                                                                 (38.897957,\n",
       "                                                                                  -77.03656)]),\n",
       "                                                  ['LATITUDE', 'LONGITUDE']),\n",
       "                                                 ('dates',\n",
       "                                                  FunctionTransformer(func=<function to_epoch at 0x7f037740da60>),\n",
       "                                                  'SALEDATE'),\n",
       "                                                 ('fill_kitchens',\n",
       "                                                  SimpleImputer(strategy='most_frequent'),\n",
       "                                                  ['KITCHENS'])])),\n",
       "                ('scaling', MaxAbsScaler()), ('regressor', Ridge())])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "To go back to the \"normal\" display, we can use `set_config(display='text')`.  \n",
    "\n",
    "Or we can get the HTML representation written to a file in this fashion (the HTML representation is what you are seeing above, rendered inside of the Jupyter notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import estimator_html_repr\n",
    "\n",
    "with open('estimator.html', 'w') as f:\n",
    "    f.write(estimator_html_repr(est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "- real-world data rarely comes in a ready-to-use format so preprocessing and data wrangling is an important part of an ML model which sometimes requires building custom functionality\n",
    "- scikit-learn predictor and transformer classes follow a particular template\n",
    "- conforming to the scikit-learn convention when building custom estimators has the benefit of being able to use custom classes together with many other scikit-learn tools (e.g. pipelines, cross-validation, grid search)\n",
    "- a scikit-learn pipeline allows for multiple transformers to be applied in succession\n",
    "- a scikit-learn feature union applies a list of transformer objects in parallel to the input data, then concatenates the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
